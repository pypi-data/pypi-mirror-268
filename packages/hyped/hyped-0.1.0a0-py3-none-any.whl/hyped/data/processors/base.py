"""Base Data Processor."""
from __future__ import annotations

from abc import ABC, abstractmethod
from itertools import chain
from types import GeneratorType
from typing import Any, ClassVar, Generator, Iterable, TypeVar

from datasets import Features
from datasets.iterable_dataset import _batch_to_examples

from hyped.base.config import BaseConfig, BaseConfigurable
from hyped.common.feature_key import FeatureKey, FeatureKeyCollection


class BaseDataProcessorConfig(BaseConfig):
    """Base Data Processor Config.

    Attributes:
        keep_input_features (bool):
            whether to pipe input features to output or only output
            features generated by the data processor
        output_format (None | dict[str, FeatureKeyCollection]):
            specifies the output feature scheme, i.e. the structure
            of the new features computed by the data processor
    """

    # specify fields that will not be parsed for feature keys
    # by default the output format is ignored as the feature keys
    # refer to output features and not required input features
    _IGNORE_KEYS_FROM_FIELDS: ClassVar[list[str]] = ["output_format"]

    # attributes
    keep_input_features: bool = True
    output_format: None | FeatureKeyCollection = None

    @property
    def required_feature_keys(self) -> Iterable[FeatureKey]:
        """Required Feature Keys.

        Iterator over all feature keys required for execution of the
        corresponding data processor.
        """

        def _iter_feature_keys(col):
            if isinstance(col, FeatureKey):
                yield col

            if isinstance(col, (list, tuple)):
                yield from chain.from_iterable(map(_iter_feature_keys, col))

            if isinstance(col, FeatureKeyCollection):
                yield from col.feature_keys

            if isinstance(col, dict):
                yield from chain.from_iterable(
                    map(_iter_feature_keys, col.values())
                )

        yield from chain.from_iterable(
            map(
                _iter_feature_keys,
                (
                    getattr(self, k)
                    for k in self.model_fields.keys()
                    if k not in type(self)._IGNORE_KEYS_FROM_FIELDS
                ),
            )
        )


T = TypeVar("T", bound=BaseDataProcessorConfig)


class BaseDataProcessor(BaseConfigurable[T], ABC):
    """Abstract Base Data Processor.

    Provides basic functionality of a data-processor. Sub-types need to
    specify the `map_features` and either the `process` or
    `internal_batch_process` function.

    Arguments:
        config (BaseDataProcessorConfig): data processor configuration
    """

    def __init__(self, config: BaseDataProcessorConfig) -> None:
        """Initialize Data Processor.

        Arguments:
            config (BaseDataProcessorConfig): processor config
        """
        self._config = config
        self._in_features: Features = None
        self._raw_features: Features = None
        self._new_features: Features = None

    @classmethod
    def from_config(cls, config: BaseDataProcessorConfig) -> BaseDataProcessor:
        """Instantiate data processor from the given config.

        Arguments:
            config (BaseDataProcessorConfig): data processor configuration
        """
        return cls(config)

    @property
    def config(self) -> BaseDataProcessorConfig:
        """Get the processor configuration.

        Returns:
            config (BaseDataProcessorConfig): config
        """
        return self._config

    @property
    def is_prepared(self) -> bool:
        """Check if the processor is prepared and ready for execution.

        Returns:
            is_prepared (bool): boolean indicating if the processor is prepared
        """
        return (self._in_features is not None) and (
            self._raw_features is not None
        )

    def prepare(self, features: Features) -> Features:
        """Prepare the processor for execution.

        Arguments:
            features (Features):
                input dataset features available to the processor on execution

        Returns:
            out_features (Features):
                dataset features of the output of the processor
        """
        # save input features
        self._in_features = features
        # map input features to output features
        # copy as preparation might disturb features inplace
        self._raw_features = Features(self.map_features(features.copy()))
        # apply output scheme to new features
        if (
            (self.config.output_format is not None)
            and isinstance(self.config.output_format, dict)
            and (len(self.config.output_format) > 0)
        ):
            self._new_features = self.config.output_format.collect_features(
                self._raw_features
            )
            assert isinstance(self._new_features, Features)
        # return output features
        return self.out_features

    @property
    def required_feature_keys(self) -> list[FeatureKey]:
        """Input dataset feature keys required for execution of the processor.

        These must be contained in the `in_features`.

        Returns:
            feature_keys (list[FeatureKey]): list of required feature keys
        """
        # TODO: make list unique
        return list(self.config.required_feature_keys)

    @property
    def in_features(self) -> Features:
        """Input dataset features available to processor.

        Returns:
            features (Features): input dataset features
        """
        # check if data processor is prepared
        if not self.is_prepared:
            raise RuntimeError(
                "Data processor not prepared. Did you forget to "
                "call `prepare` before execution?"
            )
        # return features
        return self._in_features

    @property
    def raw_features(self) -> Features:
        """New (raw) dataset features generated by the processor.

        These are the raw features before applying the `output_format`.
        If `outout_format` is not set, the `new_features` and
        `raw_features` match.

        Returns:
            features (Features): new dataset features
        """
        # check if data processor is prepared
        if not self.is_prepared:
            raise RuntimeError(
                "Data processor not prepared. Did you forget to "
                "call `prepare` before execution?"
            )
        # return features
        return self._raw_features

    @property
    def new_features(self) -> Features:
        """New dataset features generated by the processor.

        These are the features after applying the `output_format`.
        If `outout_format` is not set, the `new_features` and
        `raw_features` match.

        Returns:
            features (Features): new dataset features
        """
        # check if data processor is prepared
        if not self.is_prepared:
            raise RuntimeError(
                "Data processor not prepared. Did you forget to "
                "call `prepare` before execution?"
            )
        # return features
        return (
            self._new_features
            if self._new_features is not None
            else self._raw_features
        )

    @property
    def out_features(self) -> Features:
        """All output dataset features.

        All output features of the processor. Includes both input
        features and new features generated by the processor. On conflicts,
        the new features are prioritized.

        Returns:
            features (Features): complete output dataset features
        """
        if self.config.keep_input_features:
            return Features(self.in_features | self.new_features)
        else:
            return self.new_features

    def batch_process(
        self,
        examples: dict[str, list[Any]],
        index: list[int],
        rank: int,
        return_index: bool = False,
    ) -> dict[str, list[Any]] | tuple[dict[str, list[Any]], list[int]]:
        """Process a batch of examples.

        Arguments:
            examples (dict[str, list[Any]]): batch of examples to process
            index (list[int]): dataset indices of the examples
            rank (int): execution process rank
            return_index (bool):
                whether to return the source index for each output example

        Returns:
            out_batch (dict[str, list[Any]]): processed examples
            index (list[int]):
                the source indices to each example. Only returned when
                `return_index` is set to true.
        """
        # process batch
        out_batch, src_index = self.internal_batch_process(
            examples, index, rank
        )

        if self.config.output_format is not None:
            out_batch = self.config.output_format.collect_batch(out_batch)

        if self.config.keep_input_features:
            # check if the src index is not range(n)
            if (len(index) != len(src_index)) or any(
                i != j for i, j in enumerate(src_index)
            ):
                # gather input features to each output example
                # this filters/duplicates or reorders the input
                # features to match the output batch
                examples = {
                    k: [v[i] for i in src_index] for k, v in examples.items()
                }
                # gather the out indices
                index = [index[i] for i in src_index]

            # add input features to output batch
            out_batch = dict(examples | out_batch)

        # return output examples
        return (out_batch, index) if return_index else out_batch

    def internal_batch_process(
        self, examples: dict[str, list[Any]], index: list[int], rank: int
    ) -> tuple[dict[str, list[Any]], list[int]]:
        """Internal batch process.

        By default passes each example one-by-one to the `process`
        function and gathers the outputs into an output batch. Also
        handles the index mapping in case the `process` function
        defines a generator.

        Overwrite this function to process a whole batch simultaneously
        instead of single examples.

        Arguments:
            examples (dict[str, list[Any]]): batch of examples to process
            index (list[int]): dataset indices of the examples
            rank (int): execution process rank

        Returns:
            out_batch (dict[str, list[Any]]): processed examples
            src_index (list[int]):
                the index of the source example in the input batch that
                generated the output example, specifically the i-th output
                element is generated from the src_index[i]-th input example
        """
        out_batch = {key: [] for key in self.raw_features.keys()}
        src_index = []
        # process each example one-by-one
        for j, i, x in zip(
            range(len(index)), index, _batch_to_examples(examples)
        ):
            y = self.process(x, index=i, rank=rank)

            # handle different output types
            if isinstance(y, GeneratorType):
                pass
            elif isinstance(y, dict):
                y = (y,)
            else:
                raise ValueError(
                    "Expected output of `DataProcessor.process` to be dict or "
                    "generator of dicts, got %s" % str(y)
                )

            for d in y:
                # collect outputs
                for k, v in d.items():
                    out_batch[k].append(v)
                # add index to out index array
                src_index.append(j)

        # return output examples
        return out_batch, src_index

    def process(
        self, example: dict[str, Any], index: int, rank: int
    ) -> dict[str, Any] | Generator[dict[str, Any], None, None]:
        """Abstract process method.

        Called by `internal_batch_process`. Needs to be overwritten in
        sub-classes.

        The function can either return the output example directly, or it can
        be a generator yielding a number of generated examples. This is handy
        for data augmentation or filtering tasks. An example is filtered out
        when the generator is empty. (i.e. `yield from []`).

        Arguments:
            example (dict[str, Any]): example to process
            index (int): dataset index of the example
            rank (int): execution process rank

        Returns:
            out (dict[str, Any]|Generator[dict[str, Any], None, None]):
                processed example or generator over examples
        """
        raise NotImplementedError

    @abstractmethod
    def map_features(self, features: Features) -> Features:
        """Map dataset features.

        Map input features to *new* features. This specifies the exact
        output of the `process` function.

        Arguments:
            features (Features): input dataset features

        Returns:
            out (Features):
                new dataset features. Note that this is not the output feature
                map but only the features generated by the data processor
        """
        ...
